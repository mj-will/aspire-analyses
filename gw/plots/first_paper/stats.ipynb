{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics and Evidence\n",
    "\n",
    "This notebook collates summary statistics needed for the first paper data release. It loads the sampler outputs specified by `config.mk`, extracts run-level metrics, and emits both tables and LaTeX macros needed for the paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6197cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import yaml\n",
    "from aspire_analysis_tools.utils import read_make_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate Results and Prepare Outputs\n",
    "\n",
    "Create the `tables/` directory for exported summaries and read the `RESULTS_MAPPING` YAML pointed to by `config.mk`. Each entry in the mapping is resolved relative to the data release directory so downstream code can open the HDF5 samples without manual path edits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Path(\"tables\")\n",
    "output.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc459c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_config = read_make_config(\"config.mk\")\n",
    "results_mapping_file = Path(make_config[\"RESULTS_MAPPING\"])\n",
    "data_release_path = results_mapping_file.parent\n",
    "\n",
    "with open(results_mapping_file, \"r\") as f:\n",
    "    result_paths = yaml.safe_load(f)\n",
    "\n",
    "# Add data release path to each result path\n",
    "for key, value in result_paths.items():\n",
    "    for subkey, subvalue in value.items():\n",
    "        for subsubkey, subsubvalue in subvalue.items():\n",
    "            result_paths[key][subkey][subsubkey] = \\\n",
    "                data_release_path / Path(subsubvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Sampler Diagnostics\n",
    "\n",
    "Iterate over every analysis/waveform/sampler combination, opening the corresponding HDF5 file to record likelihood evaluations, run time, evidence values, and posterior sample counts. Missing files or keys are skipped so partially completed releases do not break the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c68fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "for label, wf_result_files in result_paths.items():\n",
    "    stats[label] = {}\n",
    "    for waveform, result_files in wf_result_files.items():\n",
    "        stats[label][waveform] = {}\n",
    "        for sampler, result_file in result_files.items():\n",
    "            stats[label][waveform][sampler] = {\n",
    "                \"evals\": None,\n",
    "                \"time\": None,\n",
    "                \"log_evidence\": None,\n",
    "                \"log_evidence_err\": None,\n",
    "                \"n_posterior_samples\": None\n",
    "            }\n",
    "            try:\n",
    "                with h5py.File(result_file, \"r\") as f:\n",
    "                    stats[label][waveform][sampler][\"evals\"] = f[\"num_likelihood_evaluations\"][()]\n",
    "                    stats[label][waveform][sampler][\"time\"] = f[\"sampling_time\"][()]\n",
    "                    stats[label][waveform][sampler][\"log_evidence\"] = f[\"log_evidence\"][()]\n",
    "                    stats[label][waveform][sampler][\"log_evidence_err\"] = f[\"log_evidence_err\"][()]\n",
    "                    stats[label][waveform][sampler][\"n_posterior_samples\"] = len(f[\"posterior/chirp_mass\"][()])\n",
    "            except (KeyError, OSError):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Summary DataFrame\n",
    "\n",
    "Flatten the nested statistics dictionary into a `pandas.DataFrame` that lists evidence, run duration, and efficiency metrics for each sampler configuration. The resulting table is displayed for quick inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04979d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table with all the evidence estimates for a given piece of data\n",
    "import pandas as pd\n",
    "df_list = []\n",
    "for label, data in stats.items():\n",
    "    for waveform, stat_dict in data.items():\n",
    "        for key, value in stat_dict.items():\n",
    "            df_list.append({\n",
    "                \"Analysis\": label,\n",
    "                \"Waveform\": waveform,\n",
    "                \"Sampler\": key,\n",
    "                \"Log Evidence\": f\"{value['log_evidence']:.2f} +/- {value['log_evidence_err']:.2f}\" if value['log_evidence'] is not None else \"N/A\",\n",
    "                \"Evals\": f\"{value['evals']:,}\" if value['evals'] is not None else \"N/A\",\n",
    "                \"Time (hours)\": f\"{value['time'] / 3600:.2f}\" if value['time'] is not None else \"N/A\",\n",
    "                \"Posterior Samples\": f\"{value['n_posterior_samples']:,}\" if value['n_posterior_samples'] is not None else \"N/A\",\n",
    "                \"Evals per Sample\": f\"{value['evals'] / value['n_posterior_samples']:.2f}\" if value['evals'] is not None and value['n_posterior_samples'] is not None else \"N/A\",\n",
    "            })\n",
    "df = pd.DataFrame(df_list)\n",
    "df = df[[\"Analysis\", \"Waveform\", \"Sampler\", \"Log Evidence\", \"Evals\", \"Time (hours)\", \"Posterior Samples\", \"Evals per Sample\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3105bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Side-by-Side Evidence Comparison\n",
    "\n",
    "Filter the summary to the analyses where both `dynesty` and `aspire` results exist, then reshape the data so the LaTeX table presents the evidences side-by-side in a consistent order with human-readable analysis labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View that only show analyses with the same waveform and different samplers\n",
    "df_grouped = df.groupby([\"Analysis\", \"Waveform\"]).filter(lambda x: len(x) > 1)\n",
    "# Convert to table with Analysis, dynesty, aspire\n",
    "# This includes:\n",
    "# GW150914-like IMRPhenomXO4a dynesty vs aspire from IMRPhenomXPHM\n",
    "# q4 IMRPhenomXO4a dynesty vs aspire from IMRPhenomXPHM\n",
    "# GW150914-like IMRPhenomPv2 dynesty vs aspire from IMRPhenomD\n",
    "# eccentric TaylorF2Ecc dynesty vs aspire from TaylorF2Ecc\n",
    "# Only show evidence\n",
    "table_df = pd.DataFrame(columns=[\"Analysis\", \"Waveform\", \"aspire\", \"dynesty\"])\n",
    "for (analysis, waveform), group in df_grouped.groupby([\"Analysis\", \"Waveform\"]):\n",
    "    dynesty_row = group[group[\"Sampler\"].str.contains(\"dynesty\")]\n",
    "    aspire_row = group[group[\"Sampler\"].str.contains(\"aspire\")]\n",
    "    if not dynesty_row.empty and not aspire_row.empty:\n",
    "        table_df = pd.concat([table_df, pd.DataFrame({\n",
    "            \"Analysis\": [analysis],\n",
    "            \"Waveform\": [waveform],\n",
    "            \"aspire\": aspire_row[\"Log Evidence\"].values[0],\n",
    "            \"dynesty\": dynesty_row[\"Log Evidence\"].values[0],\n",
    "        })], ignore_index=True)\n",
    "\n",
    "# Reorder table to match the order above\n",
    "order = [\n",
    "    (\"GW150914-like\", \"IMRPhenomXO4a\"),\n",
    "    (\"q4\", \"IMRPhenomXO4a\"),\n",
    "    (\"GW150914-like\", \"IMRPhenomPv2\"),\n",
    "    (\"eccentric\", \"TaylorF2Ecc\"),\n",
    "]\n",
    "# Change order manually\n",
    "table_df[\"Order\"] = table_df.apply(lambda row: order.index((row[\"Analysis\"], row[\"Waveform\"])) if (row[\"Analysis\"], row[\"Waveform\"]) in order else len(order), axis=1)\n",
    "table_df = table_df.sort_values(\"Order\").drop(columns=[\"Order\"])\n",
    "# Rename analysis to be more descriptive\n",
    "# Different names for the two GW150914-like analyses\n",
    "table_df[\"Analysis\"] = table_df.apply(lambda row: \"GW150914-like (IMRPhenomXO4a)\" if row[\"Analysis\"] == \"GW150914-like\" and row[\"Waveform\"] == \"IMRPhenomXO4a\" else (\"GW150914-like (IMRPhenomPv2)\" if row[\"Analysis\"] == \"GW150914-like\" and row[\"Waveform\"] == \"IMRPhenomPv2\" else (\"q4\" if row[\"Analysis\"] == \"q4\" else (\"eccentric\" if row[\"Analysis\"] == \"eccentric\" else row[\"Analysis\"]))), axis=1)\n",
    "# Replace analysis names\n",
    "table_df[\"Analysis\"] = table_df[\"Analysis\"].replace({\n",
    "    \"GW150914-like (IMRPhenomXO4a)\": \"Changing waveform (GW150914-like)\",\n",
    "    \"GW150914-like (IMRPhenomPv2)\": \"Adding spin precession\",\n",
    "    \"q4\": \"Changing waveform ($q=4$)\",\n",
    "    \"eccentric\": \"Adding orbital eccentricity\",\n",
    "    \"GW150914\": \"GW150914 (real data)\"\n",
    "})\n",
    "# Use `texttt` for waveform names\n",
    "table_df[\"Waveform\"] = table_df[\"Waveform\"].apply(lambda x: f\"\\\\texttt{{{x}}}\")\n",
    "# Use `texttt` for sampler names in column names\n",
    "table_df = table_df.rename(columns={\"dynesty\": \"\\\\texttt{dynesty}\", \"aspire\": \"\\\\texttt{aspire}\"})\n",
    "\n",
    "table_df.to_latex(output / \"evidence_table.tex\", index=False, escape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae8fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e80d4",
   "metadata": {},
   "source": [
    "## LaTeX Macros for Manuscript\n",
    "\n",
    "Create helper commands capturing the ratios of likelihood evaluations and wall-clock time per posterior sample between `dynesty` and `aspire`. The macros are written to `macros/result_macros.tex` so the paper and data release notes can reference the same numbers without manual transcriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b2232",
   "metadata": {},
   "outputs": [],
   "source": [
    "macros_output = Path(\"macros\")\n",
    "macros_output.mkdir(exist_ok=True)\n",
    "\n",
    "decimal_places = 0\n",
    "\n",
    "macros_file = macros_output / \"result_macros.tex\"\n",
    "\n",
    "# Clear the file if it exists\n",
    "with open(macros_file, \"w\") as f:\n",
    "    f.write(\"% Macros for results\\n\")\n",
    "\n",
    "def make_macro(name, value):\n",
    "    with open(macros_file, \"a\") as f:\n",
    "        # Include \\\\xspace for spacing\n",
    "        f.write(f\"\\\\newcommand{{\\\\{name}}}{{{value}\\\\xspace}}\\n\")\n",
    "\n",
    "\n",
    "# GW150914LikeEvals\n",
    "FirstDetectionLike_dynesty_evals_per_sample = stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"evals\"] / stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "FirstDetectionLike_aspire_evals_per_sample = stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"evals\"] / stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"n_posterior_samples\"]\n",
    "make_macro(\"FirstDetectionLikeEvalsPerSample\", int(np.round(FirstDetectionLike_dynesty_evals_per_sample / FirstDetectionLike_aspire_evals_per_sample, decimal_places)))\n",
    "\n",
    "# GW150914LikeTimePerSample\n",
    "FirstDetectionLike_dynesty_time_per_sample = stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"time\"] / stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "FirstDetectionLike_aspire_time_per_sample = stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"time\"] / stats[\"GW150914-like\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"n_posterior_samples\"]\n",
    "make_macro(\"FirstDetectionLikeTimePerSample\", int(np.round(FirstDetectionLike_dynesty_time_per_sample / FirstDetectionLike_aspire_time_per_sample, decimal_places)))\n",
    "\n",
    "# q4Evals\n",
    "Asymm_dynesty_evals_per_sample = stats[\"q4\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"evals\"] / stats[\"q4\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "Asymm_aspire_evals_per_sample = stats[\"q4\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"evals\"] / stats[\"q4\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"n_posterior_samples\"]\n",
    "make_macro(\"AsymmEvalsPerSample\", int(np.round(Asymm_dynesty_evals_per_sample / Asymm_aspire_evals_per_sample, decimal_places)))\n",
    "\n",
    "# AsymmTime\n",
    "Asymm_dynesty_time_per_sample = stats[\"q4\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"time\"] / stats[\"q4\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "Asymm_aspire_time_per_sample = stats[\"q4\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"time\"] / stats[\"q4\"][\"IMRPhenomXO4a\"][\"aspire from IMRPhenomXPHM\"][\"n_posterior_samples\"]\n",
    "make_macro(\"AsymmTimePerSample\", int(np.round(Asymm_dynesty_time_per_sample / Asymm_aspire_time_per_sample, decimal_places)))\n",
    "\n",
    "# eccentricEvals\n",
    "eccentric_dynesty_evals_per_sample = stats[\"eccentric\"][\"TaylorF2Ecc\"][\"dynesty\"][\"evals\"] / stats[\"eccentric\"][\"TaylorF2Ecc\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "eccentric_aspire_evals_per_sample = stats[\"eccentric\"][\"TaylorF2Ecc\"][\"aspire\"][\"evals\"] / stats[\"eccentric\"][\"TaylorF2Ecc\"][\"aspire\"][\"n_posterior_samples\"]\n",
    "make_macro(\"EccentricEvalsPerSample\", int(np.round(eccentric_dynesty_evals_per_sample / eccentric_aspire_evals_per_sample, decimal_places)))\n",
    "\n",
    "# eccentricTime\n",
    "eccentric_dynesty_time_per_sample = stats[\"eccentric\"][\"TaylorF2Ecc\"][\"dynesty\"][\"time\"] / stats[\"eccentric\"][\"TaylorF2Ecc\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "eccentric_aspire_time_per_sample = stats[\"eccentric\"][\"TaylorF2Ecc\"][\"aspire\"][\"time\"] / stats[\"eccentric\"][\"TaylorF2Ecc\"][\"aspire\"][\"n_posterior_samples\"]\n",
    "make_macro(\"EccentricTimePerSample\", int(np.round(eccentric_dynesty_time_per_sample / eccentric_aspire_time_per_sample, decimal_places)))\n",
    "\n",
    "# PrecessingEvals\n",
    "precessing_dynesty_evals_per_sample = stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"dynesty\"][\"evals\"] / stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "precessing_aspire_evals_per_sample = stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"aspire from IMRPhenomD\"][\"evals\"] / stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"aspire from IMRPhenomD\"][\"n_posterior_samples\"]\n",
    "make_macro(\"PrecessingEvalsPerSample\", int(np.round(precessing_dynesty_evals_per_sample / precessing_aspire_evals_per_sample, decimal_places)))\n",
    "\n",
    "# PrecessingTime\n",
    "precessing_dynesty_time_per_sample = stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"dynesty\"][\"time\"] / stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "precessing_aspire_time_per_sample = stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"aspire from IMRPhenomD\"][\"time\"] / stats[\"GW150914-like\"][\"IMRPhenomPv2\"][\"aspire from IMRPhenomD\"][\"n_posterior_samples\"]\n",
    "make_macro(\"PrecessingTimePerSample\", int(np.round(precessing_dynesty_time_per_sample / precessing_aspire_time_per_sample, decimal_places)))\n",
    "\n",
    "# GW150914 Evals\n",
    "FirstDetection_dynesty_evals_per_sample = stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"evals\"] / stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "FirstDetection_aspire_evals_per_sample = stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"aspire\"][\"evals\"] / stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"aspire\"][\"n_posterior_samples\"]\n",
    "make_macro(\"FirstDetectionEvalsPerSample\", int(np.round(FirstDetection_dynesty_evals_per_sample / FirstDetection_aspire_evals_per_sample, decimal_places)))\n",
    "\n",
    "# GW150914 Time\n",
    "FirstDetection_dynesty_time_per_sample = stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"time\"] / stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"dynesty\"][\"n_posterior_samples\"]\n",
    "FirstDetection_aspire_time_per_sample = stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"aspire\"][\"time\"] / stats[\"GW150914\"][\"IMRPhenomXO4a\"][\"aspire\"][\"n_posterior_samples\"]\n",
    "make_macro(\"FirstDetectionTimePerSample\", int(np.round(FirstDetection_dynesty_time_per_sample / FirstDetection_aspire_time_per_sample, decimal_places)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5a63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
